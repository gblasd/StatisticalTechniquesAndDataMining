codes <- c("italy" = 380, "canada" = 124, "egypt" = 818)
codes <- c(italy = 380, canada = 124, egypt = 818)
codes
class(codes)
names(codes) <- country
codes
seq(1, 10)
class(seq(1, 10, 0.5))
seq(1, 10, 0.5)
class(seq(1, 10, 0.5))
codes[2]
codes[c(1,3)]
codes[1:2]
codes["canada"]
codes[c["egypt", "italy"]]
codes["canada"]
codes[c("egypt", "italy")]
# entrance with the max values
max(murders$total)
# index with the max value
i_max <- which.max((murders$total))
# index with the max value
i_max <- which.max((murders$total)
i_max
# index with the max value
i_max <- which.max((murders$total))
i_max
murders$state[i_max]
murders$state[i_max] # value in the index
TRUE & TRUE
TRUE $ FALSE
TRUE & FALSE
FALSE * TRUE
TRUE * TRUE
west <- murders$region == "West"
safe <- murder_rate <= 1
murder_rate <- murders$total/ murders$population * 100000
safe <- murder_rate <= 1
safe
west <- murders$region == "West"
safe <- murder_rate <= 1
safe
ind <- safe & west
murders$state[ind]
murder_rate <- murders$total/ murders$population * 100000
murder_rate
murder_rate
murder_rate <- murders$total/ murders$population * 100000
murder_rate
west <- murders$region == "West"
west
safe
ind
murders$state[ind]
murder_rate <- murders$total/ murders$population * 100000
murder_rate
murder_rate # values
west
west
safe
#
murders$state[ind]
ind
# value by index where ind is TRUE
murders$state[ind]
murders$sate
murders$state
# value by index where ind is TRUE
murders$state[ind]
murders$state
# MATRIZ
mat <- matrix(1:12, 4, 3)
mat
# acces to data in matrix by index
mat[2,3]
mat[2, ] #
mat[, 3] # all column
mat[2, ] # see all row
mat[1:2, 2:3]
# matrix to dataframe
as.data.frame(mat)
murders[25, 1]
# matrix to dataframe
as.data.frame(mat)
data("murders")
install.packages("fastDummies")
library(fastDummies)
library(fastDummies)
dataset_dummy <- dummy_cols(data, remove_first_dummy = TRUE)
data <- read.csv("bank-full.csv", sep=";")
data
#install.packages("fastDummies")
#library(fastDummies)
dataset_dummy <- dummy_cols(data, remove_first_dummy = TRUE)
View(dataset_dummy)
dataset_dummy <- dataset_dummy[ , 18:ncol(dataset_dummy)]
dataset <- cbind(datset, dataset_dummy)
dataset <- cbind(dataset, dataset_dummy)
dataset <- cbind(data$y, dataset)
#install.packages("dpylr")
library(dplyr)
dataset <- data  %>% select_if(is.numeric)
dataset
dataset <- cbind(dataset, dataset_dummy)
View(dataset_dummy)
View(dataset_dummy)
View(dataset)
View(dataset)
data <- read.csv("bank-full.csv", sep=";")
data
str(data)
#install.packages("dpylr")
library(dplyr)
dataset <- data  %>% select_if(is.numeric)
dataset
summary(dataset)
cor(dataset)
dataset <- cbind(data$y, dataset)
colnames(dataset)[1] <- "yes"
dataset
# Split dataset into training and test set
# install.packages("caTools")
library(caTools)
set.seed(1502)
split <- sample.split(dataset$yes, SplitRatio = 0.8)
training_set <- subset(dataset, split == TRUE)
test_set <-subset(dataset, split == FALSE)
# Isolate the y variable
train.y <- as.numeric( as.factor(training_set$yes) ) -1
test.y <- as.numeric( as.factor(test_set$yes) ) -1
# Isolate the x variable
train.X <- as.matrix( training_set[, 2:ncol(training_set)])
test.X <- as.matrix( test_set[, 2:ncol(test_set)])
# state the parameters
parameters <- list(eta = 0.3,
max_depth = 6,
subsample = 1,
colsample_bytree = 1,
min_child_weight = 1,
gamma = 0,
set.seed = 1502,
eval_metric = "auc",
objective = "binary:logistic",
booster = "gbtree")
# Detect cores
# install.packages("doParallel")
library(doParallel)
detectCores()
#install.packages("xgboost")
library(xgboost)
model1 <- xgboost(data = train.X,
label = train.y,
sed.seed = 1502,
nthread = 6,
nround = 100,
params = parameters,
print_every_n = 50,
early_stopping_rounds = 10)
# predicting
predictions1 <- predict(model1, newdata = test.X)
predictions1 <- ifelse(predictions1 > 0.5, 1, 0)
#install.packages("caret")
library(caret)
confusionMatrix(table(predictions1, test.y))
#install.packages("fastDummies")
#library(fastDummies)
dataset_dummy <- dummy_cols(data, remove_first_dummy = TRUE)
#install.packages("fastDummies")
library(fastDummies)
dataset_dummy <- dummy_cols(data, remove_first_dummy = TRUE)
dataset_dummy <- dataset_dummy[ , 18:ncol(dataset_dummy)]
dataset <- cbind(dataset, dataset_dummy)
dataset <- dataset %>% select(-y_yes)
library(caTools)
set.seed(1502)
split <- sample.split(dataset$yes, SplitRatio = 0.8)
training_set <- subset(dataset, split == TRUE)
test_set <-subset(dataset, split == FALSE)
# Isolate the y variable part 2
train.y <- as.numeric( as.factor(training_set$yes) ) -1
test.y <- as.numeric( as.factor(test_set$yes) ) -1
# Isolate the x variable
train.X <- as.matrix( training_set[, 2:ncol(training_set)])
test.X <- as.matrix( test_set[, 2:ncol(test_set)])
# state the parameters
parameters <- list(eta = 0.3,
max_depth = 6,
subsample = 1,
colsample_bytree = 1,
min_child_weight = 1,
gamma = 0,
set.seed = 1502,
eval_metric = "auc",
objective = "binary:logistic",
booster = "gbtree")
# Detect cores
# install.packages("doParallel")
library(doParallel)
detectCores()
library(xgboost)
model2 <- xgboost(data = train.X,
label = train.y,
sed.seed = 1502,
nthread = 6,
nround = 100,
params = parameters,
print_every_n = 50,
early_stopping_rounds = 10)
parameters <- list(eta = 0.3,
max_depth = 6,
subsample = 1,
colsample_bytree = 1,
min_child_weight = 1,
gamma = 0,
sed.seed = 1502,
eval_metric = "auc",
objective = "binary:logistic",
booster = "gbtree")
model2 <- xgboost(data = train.X,
label = train.y,
sed.seed = 1502,
nthread = 6,
nround = 100,
params = parameters,
print_every_n = 50,
early_stopping_rounds = 10)
parameters <- list(eta = 0.3,
max_depth = 6,
subsample = 1,
colsample_bytree = 1,
min_child_weight = 1,
gamma = 0,
set.seed = 1502,
eval_metric = "auc",
objective = "binary:logistic",
booster = "gbtree")
model2 <- xgboost(data = train.X,
label = train.y,
sed.seed = 1502,
nthread = 6,
nround = 100,
params = parameters,
print_every_n = 50,
early_stopping_rounds = 10)
predictions2 <- predict(model2, newdata = test.X)
predictions2 <- ifelse(predictions2 > 0.5, 1, 0)
confusionMatrix(table(predictions2, test.y))
library(doParallel)
cpu <- makeCluster(6)
registerDoParallel(cpu)
library(doParallel)
cpu <- makeCluster(6)
registerDoParallel(cpu)
# statin the inputs
y <- as.numeric( as.factor(datset$yes) ) -1
# statin the inputs
y <- as.numeric( as.factor(dataset$yes) ) -1
X <- as.matrix( dataset[, 2:ncol(dataset)])
# control the computational nuances of the train function
tune_control <- trainControl(method = "cv",
allowParallel = TRUE,
number = 5)
tune_grid <- expand.grid(nrounds = seq(from = 50, to = 600, by = 50), eta = c(0.1, 0.2, 0.3, 0.4),
max_depth = seq(2, 10, by = 2),
subsample = c(0.5, 0.7, 1),
colsample_bytree = 1,
min_child_weight = 1,
gamma = 0,)
tune_grid <- expand.grid(nrounds = seq(from = 50, to = 600, by = 50), eta = c(0.1, 0.2, 0.3, 0.4),
max_depth = seq(from=2, to=10, by = 2),
subsample = c(0.5, 0.7, 1),
colsample_bytree = 1,
min_child_weight = 1,
gamma = 0,)
a <- c(0.1, 0.2, 0.3, 0.4)
a
tune_grid <- expand.grid(nrounds = seq(from = 50, to = 600, by = 50),
eta = c(0.1, 0.2, 0.3, 0.4),
max_depth = seq(from=2, to=10, by = 2),
subsample = c(0.5, 0.7, 1),
colsample_bytree = 1,
min_child_weight = 1,
gamma = 0)
# cross validation and parameter tunning start
start <- Sys.time()
start <- Sys.time()
xgb_tune <- train(x = X,
y = y,
method = "xgbTree",
trControl = tune_control,
tuneGrid = tune_grid)
xgb_tune%bestTune
tune_grid
# cross validation and parameter tunning start
start <- Sys.time()
xgb_tune <- train(x = X,
y = y,
method = "xgbTree",
trControl = tune_control,
tuneGrid = tune_grid)
start <- Sys.time()
xgb_tune <- train(x = X,
y = y,
method = "xgbTree",
trControl = tune_control,
tuneGrid = tune_grid)
data <- read.csv("bank-full.csv", sep=";")
data
str(data)
#install.packages("dpylr")
library(dplyr)
dataset <- data  %>% select_if(is.numeric)
dataset
summary(dataset)
cor(dataset)
dataset <- cbind(data$y, dataset)
colnames(dataset)[1] <- "yes"
dataset
# Split dataset into training and test set
# install.packages("caTools")
library(caTools)
set.seed(1502)
split <- sample.split(dataset$yes, SplitRatio = 0.8)
training_set <- subset(dataset, split == TRUE)
test_set <-subset(dataset, split == FALSE)
# Isolate the y variable
train.y <- as.numeric( as.factor(training_set$yes) ) -1
test.y <- as.numeric( as.factor(test_set$yes) ) -1
# Isolate the x variable
train.X <- as.matrix( training_set[, 2:ncol(training_set)])
test.X <- as.matrix( test_set[, 2:ncol(test_set)])
# state the parameters
parameters <- list(eta = 0.3,
max_depth = 6,
subsample = 1,
colsample_bytree = 1,
min_child_weight = 1,
gamma = 0,
set.seed = 1502,
eval_metric = "auc",
objective = "binary:logistic",
booster = "gbtree")
# Detect cores
# install.packages("doParallel")
library(doParallel)
detectCores()
#install.packages("xgboost")
library(xgboost)
model1 <- xgboost(data = train.X,
label = train.y,
sed.seed = 1502,
nthread = 6,
nround = 100,
params = parameters,
print_every_n = 50,
early_stopping_rounds = 10)
# predicting
predictions1 <- predict(model1, newdata = test.X)
predictions1 <- ifelse(predictions1 > 0.5, 1, 0)
#install.packages("caret")
library(caret)
confusionMatrix(table(predictions1, test.y))
#install.packages("fastDummies")
library(fastDummies)
dataset_dummy <- dummy_cols(data, remove_first_dummy = TRUE)
dataset_dummy <- dataset_dummy[ , 18:ncol(dataset_dummy)]
dataset <- cbind(dataset, dataset_dummy)
dataset <- dataset %>% select(-y_yes)
# Split dataset into training and test set part 2
# install.packages("caTools")
library(caTools)
set.seed(1502)
split <- sample.split(dataset$yes, SplitRatio = 0.8)
training_set <- subset(dataset, split == TRUE)
test_set <-subset(dataset, split == FALSE)
# Isolate the y variable part 2
train.y <- as.numeric( as.factor(training_set$yes) ) -1
test.y <- as.numeric( as.factor(test_set$yes) ) -1
# Isolate the x variable
train.X <- as.matrix( training_set[, 2:ncol(training_set)])
test.X <- as.matrix( test_set[, 2:ncol(test_set)])
# state the parameters
parameters <- list(eta = 0.3,
max_depth = 6,
subsample = 1,
colsample_bytree = 1,
min_child_weight = 1,
gamma = 0,
set.seed = 1502,
eval_metric = "auc",
objective = "binary:logistic",
booster = "gbtree")
# Detect cores
# install.packages("doParallel")
library(doParallel)
detectCores()
#install.packages("xgboost")
library(xgboost)
model2 <- xgboost(data = train.X,
label = train.y,
sed.seed = 1502,
nthread = 6,
nround = 100,
params = parameters,
print_every_n = 50,
early_stopping_rounds = 10)
# Predicting part 2
predictions2 <- predict(model2, newdata = test.X)
predictions2 <- ifelse(predictions2 > 0.5, 1, 0)
# Evaluate the Model (Confusion Matrix) part 2
#install.packages("caret")
#library(caret)
confusionMatrix(table(predictions2, test.y))
library(doParallel)
cpu <- makeCluster(6)
registerDoParallel(cpu)
# statin the inputs
y <- as.numeric( as.factor(dataset$yes) ) -1
X <- as.matrix( dataset[, 2:ncol(dataset)])
# control the computational nuances of the train function
tune_control <- trainControl(method = "cv",
allowParallel = TRUE,
number = 5)
tune_grid <- expand.grid(nrounds = seq(from = 50, to = 600, by = 50),
eta = c(0.1, 0.2, 0.3, 0.4),
max_depth = seq(from=2, to=10, by = 2),
subsample = c(0.5, 0.7, 1),
colsample_bytree = 1,
min_child_weight = 1,
gamma = 0)
# cross validation and parameter tunning start
#start <- Sys.time()
#xgb_tune <- train(x = X,
#                  y = y,
#                  method = "xgbTree",
#                  trControl = tune_control,
#                  tuneGrid = tune_grid)
#end <- Sys.time()
tune_grid
#xgb_tune%bestTune
#cpu <- makeCluster(6)
#registerDoParallel(cpu)
start <- Sys.time()
xgb_tune <- train(x = X,
y = y,
method = "xgbTree",
trControl = tune_control,
tuneGrid = tune_grid)
end <- Sys.time()
xgb_tune$bestTune
View(xgb_tune$results)
cpu <- makeCluster(6)
registerDoParallel(cpu)
# Set the parameters part 2
tune_grid2 <- expand.grid(nrounds = seq(from = 50, to = 600, by = 50),
eta = xgb_tune$bestTune$eta,
max_depth = xgb_tune$bestTune$max_depth,
subsample = xgb_tune$bestTune$subsample,
colsample_bytree = c(0.5, 0.7, 1),
min_child_weight = seq(1, 6, by=2),
gamma = c(0, 0.05, 0.1, 0.15))
start <- Sys.time()
xgb_tune2 <- train(x = X,
y = y,
method = "xgbTree",
trControl = tune_control,
tuneGrid = tune_grid2)
end <- Sys.time()
xgb_tune2$bestTune
View(xgb_tune2$results)
parameters3 <- list(eta = xgb_tune$bestTune$eta,
max_depth = xgb_tune$bestTune$max_depth,
subsample = xgb_tune$bestTune$subsample,
colsample_bytree = xgb_tune$bestTune$colsample_bytree,
min_child_weight = xgb_tune$bestTune$min_child_weight,
gamma = xgb_tune$bestTune$gamma,
set.seed = 1502,
eval_metric = "auc",
objective = "binary:logistic",
booster = "gbtree")
model3 <- xgboost(data = train.X,
label = train.y,
sed.seed = 1502,
nthread = 6,
nround = xgb_tune$bestTune$nrounds,
params = parameters3,
print_every_n = 50,
early_stopping_rounds = 10)
model3 <- xgboost(data = train.X,
label = train.y,
sed.seed = 1502,
nthread = 6,
nround = xgb_tune$bestTune$nrounds,
params = parameters,
print_every_n = 50,
early_stopping_rounds = 10)
predictions3 <- predict(model3, newdata = test.X)
predictions3 <- ifelse(predictions3 > 0.5, 1, 0)
confusionMatrix(table(predictions3, test.y))
importance <- xgb.importance(feature_names = colnames(test.X),
model = model3)
xgb.plot.importance(importance_matrix = importance)
xgb.plot.shap(data = test.X,
model = model3,
top_n = 3)
setwd("~/Documents/ST&DM/StatisticalTechniquesAndDataMining/AnalysisOfVarinaceFactorialAndCorrespondence")
razas<-read.table("ejemplorazas.csv",header=TRUE,sep = ',')
View(razas)
View(razas)
attach(razas)
perrosmat<-as.matrix(cbind(x1, x2, x3, x4, x5, x6))
stars(perrosmat,labels= c("Thai","Dorado","Lobo Chino","Lobo Indio","Cuon","Dingo","Prehistorioco"))
